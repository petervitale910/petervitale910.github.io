[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "All my favorite resources",
    "section": "",
    "text": "this is an h1 element\n\n\nthis is an h1 element with the title class\n\n\nthis paragraph is butterscotch\n\n\nthis paragraph is centered\n\n\nthis paragraph is centered and butterscotch\n\n\nHere’s where I’m describing something fun that I enjoy doing\n\nThis\nis\nall\nbutterscotch\n\n\nThis\nis\nall\ncentered"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a student who works"
  },
  {
    "objectID": "about.html#what-i-do-for-work",
    "href": "about.html#what-i-do-for-work",
    "title": "About",
    "section": "",
    "text": "I am currently a student who works"
  },
  {
    "objectID": "about.html#what-i-do-for-fun",
    "href": "about.html#what-i-do-for-fun",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\nLeague of Legends; give the ppl what they want."
  },
  {
    "objectID": "posts/sf_bay_catches/index.html",
    "href": "posts/sf_bay_catches/index.html",
    "title": "San Fransisco Bay Catch Dynamics",
    "section": "",
    "text": "tktktktkt\ntktktktk ### DAG\n\nImport packages\n\n\nCode\npacman::p_load('tidyverse', 'here', 'janitor', 'indicspecies', 'MASS', 'ggthemes')\n\n\n\n\nImport and clean data\n\n\nCode\n# Raw catch data\ncatch_raw &lt;- read_csv(here::here('posts', 'sf_bay_catches', 'data', 'catch_9218.csv'))\n\n# For easy reference we also import species key \nspecies_key &lt;- read_csv(here::here('posts', 'sf_bay_catches','data', 'species_key.csv'), col_names = FALSE)\n\n\nNow we can clean up the messy dataset and make a functional species key\n\n\nCode\n# The species key has some very difficult first columns so we need to restrict it down to just species code and name\nspecies_key_clean &lt;- species_key[c(4,5),3:125] %&gt;% \n  row_to_names(row_number = 1) %&gt;% \n  clean_names() %&gt;% \n  remove_empty(which = \"rows\") %&gt;% \n  pivot_longer(cols = everything(),names_to = 'species_code', values_to = 'species_name')%&gt;% \n  mutate(species_code = str_to_upper(species_code),\n                                     species_name = str_to_title(str_to_lower(species_name)))\n\nspecies_key_clean\n\n\n# A tibble: 123 × 2\n   species_code species_name          \n   &lt;chr&gt;        &lt;chr&gt;                 \n 1 AG           Arrow Goby            \n 2 AS           American Shad         \n 3 BBC          Brown Bullhead Catfish\n 4 BG           Bay Goby              \n 5 BHS          Bonyhead Sculpin      \n 6 BOC          Bocaccio Rockfish     \n 7 BP           Black Perch           \n 8 BPF          Bay Pipefish          \n 9 BR           Bat Ray               \n10 BRF          Bolinas Rockfish      \n# ℹ 113 more rows\n\n\n\n\nCode\n# The catch data is much cleaner, all we need is a quick clean names\ncatch_clean &lt;- catch_raw %&gt;% \n  clean_names()\n\ncatch_clean$species_code[is.na(catch_clean$species_code)] &lt;- 'NA'\n\nhead(catch_clean)\n\n\n# A tibble: 6 × 37\n  catch_info_trawl_id species_code  size class number sex   trawl_map_trawl_id\n                &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n1                  10 SSP              0    NA      1 &lt;NA&gt;                  10\n2                  10 SSP             90    NA      1 &lt;NA&gt;                  10\n3                  10 SSP             56    NA      1 &lt;NA&gt;                  10\n4                  10 NA              NA    NA     15 &lt;NA&gt;                  10\n5                  10 CH              NA    NA      4 &lt;NA&gt;                  10\n6                  10 BS             480    NA      1 F                     10\n# ℹ 30 more variables: locale &lt;chr&gt;, trawl_map_training_trawl &lt;lgl&gt;,\n#   trawl_map_data_audit &lt;lgl&gt;, expr1 &lt;dbl&gt;, expr2 &lt;dbl&gt;,\n#   trawl_info_trawl_id &lt;dbl&gt;, hydro_id &lt;dbl&gt;, scientists &lt;chr&gt;, intern &lt;chr&gt;,\n#   date &lt;chr&gt;, school &lt;chr&gt;, trawl_time_begin &lt;dbl&gt;, trawl_time_end &lt;dbl&gt;,\n#   trawl_time_total &lt;dbl&gt;, depth_of_trawl &lt;dbl&gt;, tidal_current &lt;chr&gt;,\n#   last_slack_time &lt;dbl&gt;, last_slack_tide &lt;chr&gt;, last_slack_height &lt;dbl&gt;,\n#   sample_station &lt;chr&gt;, water_type &lt;chr&gt;, substrate &lt;chr&gt;, …\n\n\n\n\nPreliminary analysis: Top Species\nWe want to display the top 10 species caught by quantity\n\n\nCode\ntop_10_catch &lt;- catch_clean %&gt;%\n  left_join(species_key_clean,\n            by = \"species_code\") %&gt;%\n  group_by(species_name) %&gt;%\n  summarise(total_catch = sum(number, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_catch)) %&gt;%\n  slice_head(n = 10) \n\ntop_10_catch\n\n\n# A tibble: 10 × 2\n   species_name        total_catch\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Northern Anchovy         640618\n 2 Shiner Surfperch         128489\n 3 English Sole              84714\n 4 Staghorn Sculpin          27409\n 5 Pacific Herring           25755\n 6 Speckled Sanddab          15305\n 7 California Halibut        14069\n 8 Plainfin Midshipman       13662\n 9 Sanddab Sp.               13231\n10 White Croaker             11443\n\n\nIn this analysis we will take the most abundant species, Northern Anchovy, and comparing it with indicator species from the different regions of the bay.\n\n\nPreliminary analysis: Indicator species\nIndicator species does tktktktkt. It takes a group vector and a species matrix\n\n\nCode\nindicator &lt;- catch_clean %&gt;% \n  dplyr::select(catch_info_trawl_id, species_code, number, locale) %&gt;% \n  pivot_wider(names_from = species_code, values_from = number, values_fn = sum) %&gt;% \n  mutate(across(where(is.numeric), ~replace_na(., 0))) %&gt;% \n  filter(!locale == 'BLANK')\n\ngroup_vector &lt;- indicator$locale\n\nspecies_matrix &lt;- as.matrix(indicator[, -c(1, 2)])\n\n\nNow we can run the function indval to find indicator species values for our groups\n\n\nCode\nindval &lt;- multipatt(species_matrix, group_vector, \n                    control = how(nperm=999)) \n\n\n\n\nCode\n summary &lt;- summary(indval, alpha = .1) # Alpha is significance threshold \n\n\n\n Multilevel pattern analysis\n ---------------------------\n\n Association function: IndVal.g\n Significance level (alpha): 0.1\n\n Total number of species: 162\n Selected number of species: 50 \n Number of species associated to 1 group: 33 \n Number of species associated to 2 groups: 12 \n Number of species associated to 3 groups: 5 \n\n List of species associated to each combination: \n\n Group CENTRAL BAY  #sps.  13 \n     stat p.value   \nSSD 0.456   0.006 **\nCLF 0.217   0.042 * \nPOM 0.110   0.031 * \nps  0.066   0.053 . \nHH  0.065   0.067 . \nCRL 0.065   0.082 . \nOP  0.059   0.043 * \nJM  0.059   0.035 * \nBSC 0.050   0.044 * \nes  0.046   0.096 . \nIRL 0.042   0.027 * \nssd 0.042   0.031 * \nSTR 0.029   0.086 . \n\n Group SAN PABLO BAY  #sps.  4 \n     stat p.value  \nBR  0.449   0.011 *\nDT  0.328   0.040 *\nTS  0.277   0.012 *\nBSK 0.214   0.050 *\n\n Group SOUTH BAY  #sps.  2 \n     stat p.value  \nCT  0.308   0.052 .\nDSP 0.285   0.092 .\n\n Group SUISUN BAY  #sps.  14 \n     stat p.value    \nSB  0.880   0.001 ***\nSF  0.768   0.001 ***\nTFS 0.417   0.001 ***\nSST 0.303   0.001 ***\nDLS 0.204   0.006 ** \nBDG 0.204   0.005 ** \nTP  0.148   0.014 *  \nCC  0.129   0.003 ** \nBLP 0.129   0.003 ** \nCK  0.091   0.015 *  \nKS  0.091   0.019 *  \nDS  0.089   0.021 *  \nBBC 0.088   0.021 *  \nPL  0.086   0.054 .  \n\n Group CENTRAL BAY+SAN PABLO BAY  #sps.  6 \n     stat p.value    \nPMS 0.652   0.001 ***\nPH  0.535   0.003 ** \nBG  0.495   0.001 ***\nBPF 0.338   0.014 *  \nWSP 0.320   0.017 *  \nBHS 0.225   0.094 .  \n\n Group CENTRAL BAY+SOUTH BAY  #sps.  1 \n    stat p.value  \nSD 0.381   0.031 *\n\n Group CENTRAL BAY+SUISUN BAY  #sps.  1 \n    stat p.value  \nPS 0.267   0.043 *\n\n Group SAN PABLO BAY+SOUTH BAY  #sps.  1 \n    stat p.value  \nBS 0.343   0.052 .\n\n Group SAN PABLO BAY+SUISUN BAY  #sps.  2 \n     stat p.value    \nYG  0.574   0.002 ** \nLFS 0.538   0.001 ***\n\n Group SOUTH BAY+SUISUN BAY  #sps.  1 \n   stat p.value  \nUG 0.32   0.075 .\n\n Group CENTRAL BAY+SAN PABLO BAY+SOUTH BAY  #sps.  3 \n     stat p.value   \nNA  0.811   0.002 **\nSSP 0.762   0.002 **\nWC  0.385   0.045 * \n\n Group CENTRAL BAY+SAN PABLO BAY+SUISUN BAY  #sps.  1 \n     stat p.value    \nPSD 0.527   0.001 ***\n\n Group SAN PABLO BAY+SOUTH BAY+SUISUN BAY  #sps.  1 \n    stat p.value  \nLS 0.353   0.042 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\n\nFrom the summary we can see that the top species for the four San Francisco Bay locales are:\nGroup CENTRAL BAY\nSSD: Speckled Sanddab\nGroup SAN PABLO BAY\nBR: Bat Ray\nGroup SOUTH BAY\nCT: California Tonguefish\nGroup SUISUN BAY\nSB: Striped Bass\nNext we are going to compare Northern Anchovy catches to these species using a hurdle model\n\n\nHurdle model and create function\nThe hurdle model does tktktktk\nBecause we are running and fitting this model four times, it sounds like I should make it into a function\n\n\nCode\n#' predict indicator\n#'\n#' @param df\n#' Data frame used for analysis\n#' @param sp_code \n#' Species code (wrapped in '')\n#' @param region \n#' Locale (all capitals wrapped in '')\n#' @returns\n#' a list containing: \n#' stage1_summary \n#' stage2_summary \n#' stage1_pred \n#' stage2_pred\n#' combined_pred \n#' plot \n#' @export\n#'\n#' @examples\npredict_indicator &lt;- function(df, sp_code, region){\n\nspecies_name &lt;- species_key_clean %&gt;% \n  filter(species_code == sp_code) %&gt;% \n  pull(species_name) %&gt;% \n  make_clean_names()\n  \n\n\ncatch_plot &lt;- df %&gt;% \n  filter(species_code == 'NA' | species_code == sp_code) %&gt;% \n  mutate(in_region = ifelse(locale == region, TRUE, FALSE)) %&gt;% \n  group_by(species_code, catch_info_trawl_id, in_region) %&gt;% \n  summarize(count = sum(number)) %&gt;% \n  pivot_wider(names_from = species_code,\n              values_from = count) %&gt;% \n  set_names(c('trawl_id', 'in_region', 'northern_anchovy', species_name)) \n\ncatch_plot[is.na(catch_plot)] &lt;- 0\n\na &lt;- mean(catch_plot$northern_anchovy)\nb &lt;- sd(catch_plot$northern_anchovy)\n\ncatch_plot &lt;- catch_plot %&gt;% \n  mutate(northern_anchovy_z = (northern_anchovy - a) / b,\n         presence = ifelse(.data[[species_name]] &gt; 0, 1, 0),\n         abundance = ifelse(.data[[species_name]] &gt; 0, .data[[species_name]], NA))\n\n\nm1 &lt;- glm(presence ~ northern_anchovy_z * in_region, data = catch_plot, family = binomial(link = 'logit'))\n\nm2 &lt;- glm.nb(abundance ~ northern_anchovy_z * in_region, data = catch_plot)\n\nsum_m1 &lt;- summary(m1)\n\nsum_m2 &lt;- summary(m2)\n\ngrid_data &lt;- expand_grid(in_region = c(TRUE, FALSE),\n  northern_anchovy_z = seq(min(catch_plot$northern_anchovy_z), max(catch_plot$northern_anchovy_z), by = 1))\n\npredict_pres &lt;-  predict(m1,\n                      newdata = grid_data,\n                      type = \"link\",\n                      se.fit = TRUE)\n\npredict_abu &lt;-  predict(m2,\n                      newdata = grid_data,\n                      type = \"link\",\n                      se.fit = TRUE)\n\npred_pres_result &lt;- grid_data %&gt;% \n  mutate(\n    fit_link = predict_pres$fit,\n    se = predict_pres$se.fit,\n    pred_tbs = plogis(fit_link),\n    lower = plogis(fit_link - 1.96*se),\n    upper = plogis(fit_link + 1.96*se)\n  )\n\npred_abu_result &lt;- grid_data %&gt;% \n  mutate(\n    fit_link = predict_abu$fit,\n    se = predict_abu$se.fit,\n    pred_tbs = exp(fit_link),\n    lower = exp(fit_link - 1.96*se),\n    upper = exp(fit_link + 1.96*se)\n  )\n\n# Merge predictions \ncombined &lt;- pred_pres_result %&gt;% \n  mutate(\n    expected_count = pred_tbs * pred_abu_result$pred_tbs,\n    lower_expected = lower * pred_abu_result$lower,\n    upper_expected = upper * pred_abu_result$upper\n  )\n\nplot &lt;- ggplot(combined, aes(northern_anchovy_z, expected_count))+\n  facet_wrap(~in_region,\n             labeller = as_labeller(\n      c(`TRUE` = \"In Region\", `FALSE` = \"Outside Region\")\n    ))+\n  geom_line(color = '#A0185A')+\n  geom_ribbon(aes(ymin = lower_expected, ymax = upper_expected),\n              alpha = .3, fill = '#ABC798')+\n  labs(x = 'Northern Anchovy Z',\n       y = str_to_title(str_replace_all(species_name, \"_\", \" \")))+\n  theme_clean()\n\nreturn(list(\n  stage1_summary = sum_m1,\n  stage2_summary = sum_m2,\n  stage1_pred = pred_pres_result,\n  stage2_pred = pred_abu_result,\n  combined_pred = combined,\n  plot = plot\n))\n}\n\n\n\n\nFunction call: Central Bay\nThe top indicator for the Central Bay was the Speckled Sanddab so we can call the function for that species and region\n\n\nCode\nspeckled &lt;- predict_indicator(catch_clean, 'SSD', 'CENTRAL BAY')\n\n\n`summarise()` has grouped output by 'species_code', 'catch_info_trawl_id'. You\ncan override using the `.groups` argument.\n\n\nNow speckled is encoded with our model summaries, predictions, and plot. For this I will show all outputs, in later chunks I will bypass showing predictions\n\nPresence Summary\n\n\nCode\nspeckled$stage1_summary\n\n\n\nCall:\nglm(formula = presence ~ northern_anchovy_z * in_region, family = binomial(link = \"logit\"), \n    data = catch_plot)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                      -1.55639    0.03127 -49.769  &lt; 2e-16 ***\nnorthern_anchovy_z               -0.53060    0.08660  -6.127 8.96e-10 ***\nin_regionTRUE                     1.08897    0.08073  13.490  &lt; 2e-16 ***\nnorthern_anchovy_z:in_regionTRUE  0.16045    0.17231   0.931    0.352    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 8776.2  on 8792  degrees of freedom\nResidual deviance: 8543.7  on 8789  degrees of freedom\nAIC: 8551.7\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nAbundance Summary\n\n\nCode\nspeckled$stage2_summary\n\n\n\nCall:\nglm.nb(formula = abundance ~ northern_anchovy_z * in_region, \n    data = catch_plot, init.theta = 0.7277030758, link = log)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       1.96545    0.03315  59.284   &lt;2e-16 ***\nnorthern_anchovy_z               -0.05767    0.05330  -1.082    0.279    \nin_regionTRUE                     0.81587    0.07736  10.546   &lt;2e-16 ***\nnorthern_anchovy_z:in_regionTRUE -0.10104    0.14522  -0.696    0.487    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.7277) family taken to be 1)\n\n    Null deviance: 2031.5  on 1749  degrees of freedom\nResidual deviance: 1897.4  on 1746  degrees of freedom\n  (7043 observations deleted due to missingness)\nAIC: 11014\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.7277 \n          Std. Err.:  0.0237 \n\n 2 x log-likelihood:  -11004.1330 \n\n\n\n\nPresence Prediction\n\n\nCode\nspeckled$stage1_pred \n\n\n# A tibble: 58 × 7\n   in_region northern_anchovy_z fit_link     se pred_tbs   lower upper\n   &lt;lgl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 TRUE                  -0.273   -0.367 0.0830   0.409  0.371   0.449\n 2 TRUE                   0.727   -0.737 0.134    0.324  0.269   0.384\n 3 TRUE                   1.73    -1.11  0.271    0.248  0.163   0.360\n 4 TRUE                   2.73    -1.48  0.417    0.186  0.0917  0.341\n 5 TRUE                   3.73    -1.85  0.564    0.136  0.0496  0.323\n 6 TRUE                   4.73    -2.22  0.712    0.0982 0.0263  0.305\n 7 TRUE                   5.73    -2.59  0.860    0.0700 0.0137  0.289\n 8 TRUE                   6.73    -2.96  1.01     0.0494 0.00714 0.273\n 9 TRUE                   7.73    -3.33  1.16     0.0346 0.00370 0.257\n10 TRUE                   8.73    -3.70  1.31     0.0242 0.00191 0.243\n# ℹ 48 more rows\n\n\n\n\nAbundance Prediction\n\n\nCode\nspeckled$stage2_pred\n\n\n# A tibble: 58 × 7\n   in_region northern_anchovy_z fit_link     se pred_tbs  lower upper\n   &lt;lgl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 TRUE                  -0.273     2.82 0.0756    16.9  14.5    19.5\n 2 TRUE                   0.727     2.67 0.126     14.4  11.2    18.4\n 3 TRUE                   1.73      2.51 0.250     12.3   7.51   20.0\n 4 TRUE                   2.73      2.35 0.382     10.5   4.95   22.1\n 5 TRUE                   3.73      2.19 0.515      8.93  3.25   24.5\n 6 TRUE                   4.73      2.03 0.649      7.62  2.13   27.2\n 7 TRUE                   5.73      1.87 0.784      6.50  1.40   30.2\n 8 TRUE                   6.73      1.71 0.919      5.55  0.917  33.6\n 9 TRUE                   7.73      1.55 1.05       4.73  0.601  37.3\n10 TRUE                   8.73      1.40 1.19       4.04  0.394  41.5\n# ℹ 48 more rows\n\n\n\n\nCombined Prediction\n\n\nCode\nspeckled$combined_pred\n\n\n# A tibble: 58 × 10\n   in_region northern_anchovy_z fit_link     se pred_tbs   lower upper\n   &lt;lgl&gt;                  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 TRUE                  -0.273   -0.367 0.0830   0.409  0.371   0.449\n 2 TRUE                   0.727   -0.737 0.134    0.324  0.269   0.384\n 3 TRUE                   1.73    -1.11  0.271    0.248  0.163   0.360\n 4 TRUE                   2.73    -1.48  0.417    0.186  0.0917  0.341\n 5 TRUE                   3.73    -1.85  0.564    0.136  0.0496  0.323\n 6 TRUE                   4.73    -2.22  0.712    0.0982 0.0263  0.305\n 7 TRUE                   5.73    -2.59  0.860    0.0700 0.0137  0.289\n 8 TRUE                   6.73    -2.96  1.01     0.0494 0.00714 0.273\n 9 TRUE                   7.73    -3.33  1.16     0.0346 0.00370 0.257\n10 TRUE                   8.73    -3.70  1.31     0.0242 0.00191 0.243\n# ℹ 48 more rows\n# ℹ 3 more variables: expected_count &lt;dbl&gt;, lower_expected &lt;dbl&gt;,\n#   upper_expected &lt;dbl&gt;\n\n\n\n\nSpeckled plot\n\n\nCode\nspeckled$plot\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Speckled Sanddab\ntktktktktktkttktkktktktktktktktktktk\n\n\n\nFunction call: San Pablo Bay\n\n\nCode\nbat_ray &lt;- predict_indicator(catch_clean, 'BR', 'SAN PABLO BAY')\n\n\n`summarise()` has grouped output by 'species_code', 'catch_info_trawl_id'. You\ncan override using the `.groups` argument.\n\n\n\nPresence Summary\n\n\nCode\nbat_ray$stage1_summary\n\n\n\nCall:\nglm(formula = presence ~ northern_anchovy_z * in_region, family = binomial(link = \"logit\"), \n    data = catch_plot)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       2.59814    0.04346  59.788   &lt;2e-16 ***\nnorthern_anchovy_z               -0.80069    0.03360 -23.834   &lt;2e-16 ***\nin_regionTRUE                    -1.98003    0.95992  -2.063   0.0391 *  \nnorthern_anchovy_z:in_regionTRUE  1.47628    0.93201   1.584   0.1132    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5211.7  on 8950  degrees of freedom\nResidual deviance: 4483.0  on 8947  degrees of freedom\nAIC: 4491\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nAbundance Summary\n\n\nCode\nbat_ray$stage2_summary\n\n\n\nCall:\nglm.nb(formula = abundance ~ northern_anchovy_z * in_region, \n    data = catch_plot, init.theta = 0.3735027981, link = log)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       4.32149    0.01831 236.057   &lt;2e-16 ***\nnorthern_anchovy_z               -0.22317    0.02185 -10.216   &lt;2e-16 ***\nin_regionTRUE                    -3.40921    0.91663  -3.719   0.0002 ***\nnorthern_anchovy_z:in_regionTRUE  0.27641    0.46145   0.599   0.5492    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.3735) family taken to be 1)\n\n    Null deviance: 10569  on 8188  degrees of freedom\nResidual deviance: 10476  on 8185  degrees of freedom\n  (762 observations deleted due to missingness)\nAIC: 80013\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.37350 \n          Std. Err.:  0.00480 \n\n 2 x log-likelihood:  -80002.80600 \n\n\n\n\nBat Ray Plot\n\n\nCode\nbat_ray$plot\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Bat Ray\ntktktktktktktkktktktktktktktktkt\n\n\n\nFunction Call: South Bay\n\n\nCode\nca_tonguefish &lt;- predict_indicator(catch_clean, 'CT', 'SOUTH BAY')\n\n\n`summarise()` has grouped output by 'species_code', 'catch_info_trawl_id'. You\ncan override using the `.groups` argument.\n\n\n\nPresence Summary\n\n\nCode\nca_tonguefish$stage1_summary\n\n\n\nCall:\nglm(formula = presence ~ northern_anchovy_z * in_region, family = binomial(link = \"logit\"), \n    data = catch_plot)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        4.6548     1.0872   4.281 1.86e-05 ***\nnorthern_anchovy_z               -16.4148     3.8546  -4.258 2.06e-05 ***\nin_regionTRUE                     -0.9482     1.0898  -0.870    0.384    \nnorthern_anchovy_z:in_regionTRUE  16.2278     3.8548   4.210 2.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1852.3  on 8383  degrees of freedom\nResidual deviance: 1774.7  on 8380  degrees of freedom\nAIC: 1782.7\n\nNumber of Fisher Scoring iterations: 9\n\n\n\n\nAbundance Summary\n\n\nCode\nca_tonguefish$stage2_summary\n\n\n\nCall:\nglm.nb(formula = abundance ~ northern_anchovy_z * in_region, \n    data = catch_plot, init.theta = 0.3724877985, link = log)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        3.8710     0.3475  11.140   &lt;2e-16 ***\nnorthern_anchovy_z                -3.8196     1.9846  -1.925   0.0543 .  \nin_regionTRUE                      0.4629     0.3480   1.330   0.1834    \nnorthern_anchovy_z:in_regionTRUE   3.6892     1.9847   1.859   0.0631 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.3725) family taken to be 1)\n\n    Null deviance: 10540  on 8188  degrees of freedom\nResidual deviance: 10483  on 8185  degrees of freedom\n  (195 observations deleted due to missingness)\nAIC: 80048\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.37249 \n          Std. Err.:  0.00478 \n\n 2 x log-likelihood:  -80037.67300 \n\n\n\n\nTonguefish Plot\n\n\nCode\nca_tonguefish$plot\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: California Tonguefish\ntktktktktk\n\n\n\nFunction Call: Suisun Bay\n\n\nCode\nbass &lt;- predict_indicator(catch_clean, 'SB', 'SUISUN BAY')\n\n\n`summarise()` has grouped output by 'species_code', 'catch_info_trawl_id'. You\ncan override using the `.groups` argument.\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning in predict.lm(object, newdata, se.fit, scale = residual.scale, type =\nif (type == : prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\n\nPresence Summary\n\n\nCode\nbass$stage1_summary\n\n\n\nCall:\nglm(formula = presence ~ northern_anchovy_z * in_region, family = binomial(link = \"logit\"), \n    data = catch_plot)\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                      -3.647e+00  1.208e-01 -30.195  &lt; 2e-16 ***\nnorthern_anchovy_z               -2.844e+00  4.959e-01  -5.735 9.76e-09 ***\nin_regionTRUE                    -2.500e+03  1.834e+05  -0.014    0.989    \nnorthern_anchovy_z:in_regionTRUE -9.033e+03  6.577e+05  -0.014    0.989    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3454.3  on 8422  degrees of freedom\nResidual deviance: 2756.5  on 8419  degrees of freedom\nAIC: 2764.5\n\nNumber of Fisher Scoring iterations: 15\n\n\n\n\nAbundance Summary\n\n\nCode\nbass$stage2_summary\n\n\n\nCall:\nglm.nb(formula = abundance ~ northern_anchovy_z * in_region, \n    data = catch_plot, init.theta = 3.09286128, link = log)\n\nCoefficients: (1 not defined because of singularities)\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       0.33019    0.09293   3.553 0.000381 ***\nnorthern_anchovy_z               -0.52114    0.33331  -1.563 0.117935    \nin_regionTRUE                     1.41003    0.08870  15.896  &lt; 2e-16 ***\nnorthern_anchovy_z:in_regionTRUE       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.0929) family taken to be 1)\n\n    Null deviance: 618.93  on 439  degrees of freedom\nResidual deviance: 324.65  on 437  degrees of freedom\n  (7983 observations deleted due to missingness)\nAIC: 1658.2\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.093 \n          Std. Err.:  0.380 \n\n 2 x log-likelihood:  -1650.219 \n\n\n\n\nBass Plot\n\n\nCode\nbass$plot\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Striped Bass\ntktktktktk\n\n\n\nSimulate data\ntktkttktktktk\n\n\nFinal Discussion\ntktktktktktktk\n\n\n\n\nCitationBibTeX citation:@online{vitale2025,\n  author = {Vitale, Peter},\n  title = {San {Fransisco} {Bay} {Catch} {Dynamics}},\n  date = {2025-12-03},\n  url = {https://petervitale910.github.io/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVitale, Peter. 2025. “San Fransisco Bay Catch Dynamics.”\nDecember 3, 2025. https://petervitale910.github.io/."
  },
  {
    "objectID": "posts/extreme_weather_events/index.html",
    "href": "posts/extreme_weather_events/index.html",
    "title": "Extreme Weather Events",
    "section": "",
    "text": "This is where the body of my blog post begins! I’m going to add a footnote here 1 Here’s and inline footnote2\nCiting reference (Shanny-Csik 2022)\nlets cite a paper using the DOI (Gaynor et al. 2022)"
  },
  {
    "objectID": "posts/extreme_weather_events/index.html#footnotes",
    "href": "posts/extreme_weather_events/index.html#footnotes",
    "title": "Extreme Weather Events",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is some additional context that might be helpful↩︎\nhere’s my inline note↩︎"
  },
  {
    "objectID": "posts/palisades_eaton_ejscreen/index.html",
    "href": "posts/palisades_eaton_ejscreen/index.html",
    "title": "Visualizing the 2025 California Wildfires",
    "section": "",
    "text": "Guillen (2025)"
  },
  {
    "objectID": "posts/palisades_eaton_ejscreen/index.html#about-this-project",
    "href": "posts/palisades_eaton_ejscreen/index.html#about-this-project",
    "title": "Visualizing the 2025 California Wildfires",
    "section": "About this project",
    "text": "About this project\nIn this project we will be using true and false color imagery to examine the impacts of two fires which struck the Los Angeles area in early 2025.\n\nHighlights:\n\nExtracting color bands from xarray\nMapping using true and false color imagery\nOverlaying fire perimiters to see effect of fires\n\n\n\nBackground information\nThe Palisades and Eaton fires sparked within hours of each other on January 7th 2025. Due to arid conditions and the Santa Ana winds these fires quickly grew out of control, ultimately burning a combined 40,000 acres (VanAuken 2025). The fires led to significant damage and economic loss, estimated between $135 billion and $150 billion (“Media Advisory: AccuWeather Increases Estimate of Total Damage and Economic Loss as Catastrophic Wildfires Continue to Ravage the Los Angeles Area” 2025). In this project we will examine the total land area covered by the fires as well as examine socioeconomic factors to interpret the communities affected by, as well as the long lasting effects of the fires.\n\n\nAbout the data:\n\nPalisades and Eaton fire perimeters\nThese contain dissolved fire perimeters/boundaries for Eaton and Palisades fires, with boundary polygons dissolved for each fire to create a single fire burn perimeter. Access given by the County of Los Angeles (“Palisades and Eaton Dissolved Fire Perimeters (2025)” 2025)\n\n\nNetcdf landsat data\nLandsat C2 L2 from Microsoft’s Planetary Computer is a globally available, multi-decadal archive of atmospherically corrected, analysis-ready Landsat imagery. Surface reflectance for multispectral bands and derived land surface temperature from thermal bands are provided in cloud-optimized GeoTIFFs, accessible via a STAC API. Accordingly, it is primarily used for long-term environmental monitoring, spectral studies, and thermal remote sensing, and is designed for scalable, cloud-native remote sensing workflows (U.S. Geological Survey 2025)\n\n\n\n1. Import packages and datasets\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd \nimport matplotlib.pyplot as plt\nimport os \nimport xarray as xr\nimport rioxarray as rio\nimport numpy as np \nfrom matplotlib_scalebar.scalebar import ScaleBar\nimport contextily as ctx\n\n\n\n\n\n2. Fire perimeter data exploration\nHere we import the perimeter data files and do some exploration.\nI am going to import both perimeters and then look at the CRS, or coordinate reference system. This allows us to map all datasets using the same projection.\n\n\nCode\nfp = os.path.join('data','Eaton_Perimeter_20250121','Eaton_Perimeter_20250121.shp')\neaton_perimeter = gpd.read_file(fp)\n\n\n\n\nCode\nfp = os.path.join('data','Palisades_Perimeter_20250121','Palisades_Perimeter_20250121.shp')\npalisades_perimeter = gpd.read_file(fp)\n\n\n\n\nCode\n# Now we can look at the crs's and do some exploration \npalisades_perimeter.crs\n\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n\nCode\neaton_perimeter.crs\n\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nWe know we want to merge these datasets at some point, so it is important to check if the column names match in the two perimeters\n\n\nCode\n# Lets compare the column names to make sure they match \n\nprint(f' It is {eaton_perimeter.columns == palisades_perimeter.columns} that the column names match')\n\n\n It is [ True  True  True  True  True] that he column names match\n\n\n\n\nCode\n# Thats great we only need to look at one of the dataframes columns \n\nprint(' The column names of the data frames are', eaton_perimeter.columns)\n\n\n The column names of the data frames are Index(['OBJECTID', 'type', 'Shape__Are', 'Shape__Len', 'geometry'], dtype='object')\n\n\nSummary: From the preliminary exploration both the Palisades and Eaton datasets are in ESPG:3857, which is a projected coordinate system. Both have the same column names which will make analysis and merging easier.\n\n\n3. NetCDF data import and exploration\nHere we will import and explore the Landsat data using xr.open_dataset().\n\n\nCode\nnetcdf = xr.open_dataset('data/landsat8-2025-02-23-palisades-eaton.nc')\n\n\n\n\nCode\nnetcdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 78MB\nDimensions:      (y: 1418, x: 2742)\nCoordinates:\n  * y            (y) float64 11kB 3.799e+06 3.799e+06 ... 3.757e+06 3.757e+06\n  * x            (x) float64 22kB 3.344e+05 3.344e+05 ... 4.166e+05 4.166e+05\n    time         datetime64[ns] 8B ...\nData variables:\n    red          (y, x) float32 16MB ...\n    green        (y, x) float32 16MB ...\n    blue         (y, x) float32 16MB ...\n    nir08        (y, x) float32 16MB ...\n    swir22       (y, x) float32 16MB ...\n    spatial_ref  int64 8B ...xarray.DatasetDimensions:y: 1418x: 2742Coordinates: (3)y(y)float643.799e+06 3.799e+06 ... 3.757e+06units :metreresolution :-30.0crs :EPSG:32611axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinatearray([3799050., 3799020., 3798990., ..., 3756600., 3756570., 3756540.])x(x)float643.344e+05 3.344e+05 ... 4.166e+05units :metreresolution :30.0crs :EPSG:32611axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinatearray([334410., 334440., 334470., ..., 416580., 416610., 416640.])time()datetime64[ns]...[1 values with dtype=datetime64[ns]]Data variables: (6)red(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]green(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]blue(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]nir08(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]swir22(y, x)float32...grid_mapping :spatial_ref[3888156 values with dtype=float32]spatial_ref()int64...crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :334395.0 30.0 0.0 3799065.0 0.0 -30.0[1 values with dtype=int64]Indexes: (2)yPandasIndexPandasIndex(Index([3799050.0, 3799020.0, 3798990.0, 3798960.0, 3798930.0, 3798900.0,\n       3798870.0, 3798840.0, 3798810.0, 3798780.0,\n       ...\n       3756810.0, 3756780.0, 3756750.0, 3756720.0, 3756690.0, 3756660.0,\n       3756630.0, 3756600.0, 3756570.0, 3756540.0],\n      dtype='float64', name='y', length=1418))xPandasIndexPandasIndex(Index([334410.0, 334440.0, 334470.0, 334500.0, 334530.0, 334560.0, 334590.0,\n       334620.0, 334650.0, 334680.0,\n       ...\n       416370.0, 416400.0, 416430.0, 416460.0, 416490.0, 416520.0, 416550.0,\n       416580.0, 416610.0, 416640.0],\n      dtype='float64', name='x', length=2742))Attributes: (0)\n\n\nThis xarray has 1418 y dimensions and 2742 x dimensions. The coordinates are x, y, and time which are are float64, float64, and datetime64 types (respectively). The variables are red, green, blue, near infared and short wave infared light bands as well as a spatial reference. The bands are in float32 while the spatial_ref is an int64, this shouldn’t be an issue going further - however as the analysis continues it may become an issue.\n\n\n4. Restoring geospatial information\nWe need to use rio.crs to print what is the CRS of this dataset, which will help us plot the light bands onto the same area as the fire perimeters.\n\n\nCode\nprint(f'The xarray has the crs: {netcdf.rio.crs}')\n\n\nThe xarray has the crs: None\n\n\nOh no! Theres no crs meaning that it will be harder to plot, however there actually is! It’s just encoded in the spatial_ref variable.\n\n\nCode\nprint(f' The actual crs of the xarray is: {netcdf.spatial_ref.crs_wkt}')\n\n\n The actual crs of the xarray is: PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]\n\n\nOh wow thats big! Lets make it so that crs is the crs for the dataset\n\n\nCode\nnetcdf = netcdf.rio.write_crs(netcdf.spatial_ref.crs_wkt) # Need to write_crs due to being an xarrray\n\n\n\n\nCode\nprint(f'The xarray now has the crs: {netcdf.rio.crs}')\n\n\nThe xarray now has the crs: EPSG:32611\n\n\n\n\n5. True color image\nA true color image takes the light bands collected by landsat and plots the colors visible to the human eye. This is how the ground would look if we were sitting on the satelite\nTo create a true color plot we need to:\n\nIdentify which bands have nan values.\nSelect the red, green, and blue variables (in that order) of the xarray.Dataset holding the Landsat data,\nConvert it to a numpy.array using the to_array() method, and then\nUse .plot.imshow() to create an RGB image with the data.\nAdjust the scale used for plotting the bands to get a true color image.\nUse the .fillna() method for xarray.Datasets to substitute any nan values in the Landsat data for zero.\nCreate a true color image that gets plotted without warnings.\n\n\n\nCode\nprint(f'The red band has {np.isnan(netcdf.red).sum().item()} na values')\nprint(f'The green band has {np.isnan(netcdf.green).sum().item()} na values')\nprint(f'The blue band has {np.isnan(netcdf.blue).sum().item()} na values')\n\n\nThe red band has 0 na values\nThe green band has 1 na values\nThe blue band has 109 na values\n\n\n\n\nCode\nnetcdf[['red', 'green', 'blue']].fillna(0).to_array().plot.imshow(robust = True) # Fillna and robust = true are the fix for the two problems\n\n\n\n\n\n\n\n\n\nYou can sort of see the fire damage, however it isnt as aparant as the false color image.\n\n\n6. False color image\nFalse color images plot short wave infared, near infared, and red bands. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies (“Assignment 4: Thomas Fire — MEDS-EDS-220 Course” 2025)\n\n\nCode\n# This plotting code is very similar to the other, but we are selecting bands\nnetcdf[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True) # Note! No Na's are present\n\n\n\n\n\n\n\n\n\nThe red areas are burn scars from the fires, and to see them better we can plot the fire perimeters on top.\n\n\n7. Map\nHere we map the fire perimeters onto the false color image. Before we plot we need to make sure that the crs matches\n\n\nCode\n# Before we plot we need to ensure we are in the same crs\npalisades_perimeter = palisades_perimeter.to_crs(netcdf.rio.crs)\n\n\neaton_perimeter = eaton_perimeter.to_crs(netcdf.rio.crs)\n\n\n\n\nCode\n \n\nfig, ax = plt.subplots(1, 1, figsize = (14,12)) # Set a size for our figure\n\nnetcdf[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True, ax = ax) # Choose false color bands\n\neaton_perimeter.plot(ax = ax, color = 'none', edgecolor = 'red') # Plot Eaton fire perimeter in red\n\npalisades_perimeter.plot(ax = ax, color = 'none', edgecolor = 'red') # And the Palisade perimeter\n\n\nplt.suptitle('False Color image of Palisades and Eaton fires in Santa Barbara', \n             fontsize=16, \n             y = .77) # This y was a lot of guess and checking \nplt.title(\"Plot of SWIR, NIR, and RED light bands\", \n          fontsize=12, \n          y=1.01) # Title defaults inside the plot and I want it just above, hence 1.01\n\n\n\nplt.figtext(x = .2, # The position of the figtext, In units of 0-1 for x and y \n            y = .46,\n            s = 'Palisades Fire Perimeter',\n            color = 'black',\n            bbox ={'facecolor':'grey',  # Add a box around text for legibility \n                   'alpha':1, 'pad':5}) \nplt.figtext(x = .67, \n            y = .63, \n            s= 'Eaton Fire Perimeter',\n            color = 'black',\n           bbox ={'facecolor':'grey', \n                   'alpha':1, 'pad':5})\n\nax.add_artist(ScaleBar(1, dimension=\"si-length\", units=\"m\", location=\"lower left\")) # Add a scale bar on the lower left\n\nax.axis('off') # Turn Axes off\n\nplt.show() # Show plot only \n\n\n\n\n\n\n\n\n\nShort-wave infrared (SWIR) was mapped to the red channel, near-infrared (NIR) to green, and the red band to blue. This false-color composite highlights post-fire effects from the January 2025 Palisades–Eaton fires. Burned areas appear as bright orange to deep red, while the fire perimeter is outlined in red for clarity, with the fire name next to the perimeter. As you can see some of the fire perimeter still contains some vegitation, thanks to our brave firefighters.\n\n\n8. Intersection with environmental justice\nIn this and the following sections we will be accessing data from the California Environmental Justice Index (EJI), and plotting census count information in the context of the fire perimeters.\nWe begin by importing the EJI. (Centers for Disease Control and Prevention and Agency for Toxic Substances and Disease Registry 2021)\n\n\nCode\neji_ca = gpd.read_file('/Users/petervitale/Desktop/MEDS/Career/petervitale910.github.io/posts/palisades_eaton_ejscreen/data/EJI_2024_California /EJI_2024_California.gdb')\n\n\nThe EJI has a number of socioeconomic factors along with geometries of where the census was taken. These include:\n\nPercentage of housing units that are renter occupied: E_RENTER\nPercentage of persons who are unisured: E_UNINSUR\n65+: E_AGE65\nproportion of greenspace: E_PARK\n\n\n\n9. Polygon intersection\nTo find where social variables intersect our fire perimeters we can use polygon intersection, where we examine where the shape of the census tracts intersect with the shape of the fire perimeter.\n\n\nCode\n# From previous exploration I know the crs' do not match \n# Spatially join EJI with palisades\npali_eji = gpd.sjoin(eji_ca.to_crs(palisades_perimeter.crs), palisades_perimeter) \n\nfig, ax = plt.subplots(figsize = (11,5))\nax.axis('off')\npali_eji.plot('TRACTCE',\n              ax = ax)\n\npalisades_perimeter.plot(ax = ax,\n                    color = 'none',\n                    edgecolor = 'red',\n                    linewidth = 1.5)\n\n\n\n\n\n\n\n\n\nPolygon intersection includes the entire census tracts that are touching the fire, see orange. This problem becomes more aparent when we plot the Eaton fire.\n\n\nCode\n# Eaton fire\n\n# From previous exploration i know the crs do not match \n# Spatially join EJI with palisades\neaton_eji = gpd.sjoin(eji_ca.to_crs(eaton_perimeter.crs), eaton_perimeter) \n\nfig, ax = plt.subplots(figsize = (11,5))\nax.axis('off')\neaton_eji.plot('TRACTCE',\n              ax = ax)\n\neaton_perimeter.plot(ax = ax,\n                    color = 'none',\n                    edgecolor = 'red',\n                    linewidth = 1.5)\n\n\n\n\n\n\n\n\n\nAs you can see a large census tract is completely included, despite a comparably small portion of the tract having been effected by the fire.\n\n\n10. Polygon clipping\nInstead of polygon intersection we will be using polygon clipping. If you think of cookies, polygon intersection is like the dalgona cookies from squid games. The shape is almost cut out, however the larger area is still aparent. Polygon clipping is more like using cookie cutter to only choose the area from fire perimeter, not the entire census.\nFor future plotting we are going to make new, clipped variables.\n\n\nCode\neji_ca = eji_ca.to_crs(palisades_perimeter.crs)\nclip_eji_pali = gpd.clip(eji_ca, palisades_perimeter)\nclip_eji_eaton = gpd.clip(eji_ca, eaton_perimeter)\n\n\nAs you can see the clipped plots show the exact areas of the census tracts which were in the fire perimeters.\n\n\nCode\nclip_eji_pali.plot('TRACTCE')\n\n\n\n\n\n\n\n\n\n\n\nCode\nclip_eji_eaton.plot('TRACTCE')\n\n\n\n\n\n\n\n\n\n\n\n11. Visualize fire perimeters with a basemap\nUsing the package ctx we can add a base map to contextualize the fire areas within the greater Los Angeles Area.\n\n\nCode\nfig, ax = plt.subplots(1, 1, figsize=(14, 12))\nax.ticklabel_format(style='plain')\n\n\n# 1. Reproject everything to EPSG:3857\nclip_eji_pali_3857      = clip_eji_pali.to_crs(epsg=3857)\nclip_eji_eaton_3857     = clip_eji_eaton.to_crs(epsg=3857)\npalisades_perimeter_3857 = palisades_perimeter.to_crs(epsg=3857)\neaton_perimeter_3857     = eaton_perimeter.to_crs(epsg=3857)\n\n# 2. Plot layers\nclip_eji_pali_3857.plot(\n    ax=ax, facecolor='red', alpha=0.6, edgecolor='none'\n)\nclip_eji_eaton_3857.plot(\n    ax=ax, facecolor='blue', alpha=0.4, edgecolor='none'\n)\n\npalisades_perimeter_3857.plot(\n    ax=ax, color='none', edgecolor='black', linewidth=2\n)\neaton_perimeter_3857.plot(\n    ax=ax, color='none', edgecolor='purple', linewidth=2\n)\n\n# 3. Add basemap last\nctx.add_basemap(ax=ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot turns highlights the neighborhoods which were effected, as well as showing just how close to more densly populated areas the fires got.\n\n\n12. Mapping socioecomic factors\nFor this section I chose to examine the percentage of the census tracts which were renters. I chose this because, following the fires, renters faced issues with: - Loss of homes - Rent increases (double to triple the price) - Landlords trying to push out long-term renters who pay lower rents to make way for displaced fire victims who have more money to spend - Paying out of pocket to clean and fix partially damaged rental space (CBS Los Angeles 2025)\nThese issues compound the already devistating fire and reduce quality of life for renters.\nTo plot this I used the percentage of census tracts which are renters.\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 12))\n\n# UPDATE WITH YOU EJI VARIABLE FROM STEP 1\neji_variable = 'E_RENTER'\n\n# Find common min/max for legend range\nvmin = min(clip_eji_pali[eji_variable].min(), clip_eji_eaton[eji_variable].min())\nvmax = max(clip_eji_pali[eji_variable].max(), clip_eji_eaton[eji_variable].max())\n\n# Plot census tracts within Palisades perimeter\nclip_eji_pali.plot(\n    column= eji_variable,\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax1,\n)\nax1.set_title('Palisades Fire')\nax1.axis('off')\n\n# Plot census tracts within Eaton perimeter\nclip_eji_eaton.plot(\n    column=eji_variable,\n    vmin=vmin, vmax=vmax,\n    legend=False,\n    ax=ax2,\n)\nax2.set_title('Eaton Fire')\nax2.axis('off')\n\n# Add overall title\nfig.suptitle('Renter occupied buildings in Palisades and Eaton Fires')\n\n# Add shared colorbar at the bottom\nsm = plt.cm.ScalarMappable( norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncbar_ax = fig.add_axes([0.25, 0.08, 0.5, 0.02])  # [left, bottom, width, height]\ncbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\ncbar.set_label('Percent Renters')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom the map you can see that the Palisades fire affected more renters, with a large area of close to 50% renters being found at the lower left of the perimeter (Malibu). While this is an affluent area, it has seen some of the highest level of post-fire price gouging (FOX 11 Los Angeles 2025). This could alienate those who have historically lived in the communities to move elsewhere, shifting the social and demographic makeup of these areas."
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html",
    "href": "posts/aquaculture_and_eezs/index.html",
    "title": "Aquaculture and EEZ’s",
    "section": "",
    "text": "DESCRIPTION OF EEZ”S AND WHY I LIKE THIS\nCode\npacman::p_load('tidyverse', \n               'sf', \n               'here',\n               'tmap', \n               'kableExtra',\n               'patchwork',\n               'stars',\n               'terra')"
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html#data-import",
    "href": "posts/aquaculture_and_eezs/index.html#data-import",
    "title": "Aquaculture and EEZ’s",
    "section": "Data import",
    "text": "Data import\n\nSurface temperature from: [@noaa_crw_sst31]\n\n\nCode\nsst_2008 &lt;- rast(here('posts', 'aquaculture_and_eezs','data',\n                      'average_annual_sst_2008.tif')) \n\nsst_2009 &lt;- rast(here('posts', 'aquaculture_and_eezs','data',\n                      'average_annual_sst_2009.tif'))\nsst_2010 &lt;- rast(here('posts', 'aquaculture_and_eezs', 'data',\n                      'average_annual_sst_2010.tif'))\nsst_2011 &lt;- rast(here('posts', 'aquaculture_and_eezs','data',\n                      'average_annual_sst_2011.tif'))\nsst_2012 &lt;- rast(here('posts', 'aquaculture_and_eezs', 'data',\n                      'average_annual_sst_2012.tif'))\n\n\n\n\nBathymetry\nFrom: [@gebcobathymetriccompilationgroup20252025]\n\n\nCode\nbathy &lt;- rast(here('posts', 'aquaculture_and_eezs', 'data','depth.tif')) %&gt;% \n  project(\"EPSG:4326\") # Coerce into correct crs \n\n\n\n\nExclusive Economic Zones\nFrom: [@marine_regions_eez_v12]\n\n\nCode\neez &lt;- st_read(here('posts', 'aquaculture_and_eezs','data',\n                    'wc_regions_clean.shp'), quiet = TRUE) %&gt;% \n  st_transform(\"EPSG:4326\") # STARS OBJECT"
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html#surface-temperature-mean",
    "href": "posts/aquaculture_and_eezs/index.html#surface-temperature-mean",
    "title": "Aquaculture and EEZ’s",
    "section": "Surface temperature mean",
    "text": "Surface temperature mean\n\n\nCode\n# Stacked raster\nstacked_sst &lt;- c(sst_2008, sst_2009, sst_2010, sst_2011, sst_2012) %&gt;% # Here we stack our temperatures and then transform crs\n  project(\"EPSG:4326\") \n\n# Take mean of stacked raster and transform from Kelvin to Celsius\nmean_sst &lt;- mean(stacked_sst) - 273.15"
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html#bathymetry-1",
    "href": "posts/aquaculture_and_eezs/index.html#bathymetry-1",
    "title": "Aquaculture and EEZ’s",
    "section": "Bathymetry",
    "text": "Bathymetry\n\n\nCode\n# First we need to assure this data is in the same crs as sst\ndepth_projected &lt;- project(bathy, mean_sst) # Project the crs of mean sst on depth \n\ncrs(depth_projected) == crs(mean_sst)\n\n\n[1] TRUE\n\n\nCode\n# We are going to crop the depth raster to the eez\ndepth_cropped &lt;- crop(depth_projected, mean_sst)\n\n# And resample to make sure our pixel sizes match \ndepth_resampled &lt;- resample(depth_projected, mean_sst)"
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html#oyster-reclassification",
    "href": "posts/aquaculture_and_eezs/index.html#oyster-reclassification",
    "title": "Aquaculture and EEZ’s",
    "section": "Oyster Reclassification",
    "text": "Oyster Reclassification\nWe need to reclassify for oysters, which enjoy depth between -70 meters and 0 meters and temperatures between 11 and 30 degrees celcius\n\n\nCode\nrcl_depth &lt;- matrix(c(-Inf, -70, 0, # Anything below 70 m becomes a 0\n                    -70, 0, 1, # Between 70 and 0 becomes 1\n                    0, Inf, 0), # And above 0 becomes a 0\n                    nrow = 3, \n                    byrow = TRUE)\n\nrcl_temp &lt;- matrix(c(-Inf, 11, 0, # Below 11 degrees becomes 0\n                     11, 30, 1, # Between 11 and 30 becomes 1\n                     30, Inf, 0), # Above 30 becomes 0\n                   nrow =3,\n                   byrow = TRUE)\n# Apply those matrices\ndepth_rcl &lt;- classify(depth_resampled, rcl_depth) \n\nsst_rcl &lt;- classify(mean_sst, rcl_temp)\n\n\nWe want to keep where both sst and depth equal one, so we can multiply the two rasters.\n\n\nCode\nsuitable_cells &lt;- sst_rcl * depth_rcl\n\n# And make all 0's NA\nsuitable_cells[suitable_cells == 0] &lt;- NA\n\n# Finally, since we only have ones, change that to suitable\nnames(suitable_cells) &lt;- 'Suitable'\n\n\n\n\nCode\n# Now we want to see the area of the suitable cells, which we can get by multiplying suitable_cells and the cellSize of each cell\narea_raster &lt;- suitable_cells * cellSize(suitable_cells, unit = 'km')  \n\n\n\n\nCode\n# Here we extract where that area touches our eez\narea_extracted &lt;- terra::extract(area_raster, \n                                 eez, \n                                 touches = TRUE) %&gt;%  \n  group_by(ID) %&gt;% # Then we group_by and summarise the total suitable area per eez region\n  summarise(suitable_area = sum(Suitable, na.rm = TRUE)) %&gt;% \n  mutate(rgn = eez$rgn)\n\n\n\n\nCode\n# Lastly we want to left join to preserve the area information of each eez region\nregion_suitable_area &lt;- left_join(x = eez,\n                                  y = area_extracted, \n                                  by = \"rgn\") %&gt;% \n  select(-ID) # Get rid of ID column\n\n\n\nMap\nNow we have our final mapping data frame and can build our map\n\n\nCode\ntm_basemap(\"Esri.WorldTopoMap\")+# I found this basemap fit the best\ntm_shape(region_suitable_area)+\n  tm_polygons(fill = 'suitable_area',\n              fill.scale = tm_scale(values = c('#E1DABD', # Add my color palette \n                                              '#ABC798',\n                                              '#FFC4EB',\n                                              '#E6983F',\n                                              '#A0185A'),\n                                    breaks = c(0, \n                                               800, # Specify breaks \n                                               1600,\n                                               2400, \n                                               3200,\n                                               4000)),\n              fill.legend = tm_legend( '        Total \\nSuitable area (km)'))+ # The blank space makes 'Total' centered\n  tm_title(text = paste('Suitable area for farming Oysters in West Coast EEZ'), \n           position = tm_pos_out(\"center\", \"top\", pos.h = \"center\"))+ # Add title and position in center\n  tm_layout(component.autoscale = FALSE)+ # Autoscale makes our title tiny so we turn it off\n  tm_graticules(alpha = .5)+ # Add graticules (lighter)\n  tm_text('rgn', size = .7)+ # This adds the region name on each polygon centroid\ntm_shape(suitable_cells)+ # I want to add the visual representation of where the suitable area is \n  tm_raster(col  =  'Suitable',\n             col.scale =  tm_scale(values = '#90AFDA', # This color is also part of my palette\n            labels = 'Suitable Area'), # In place of a title  we will be specific with our legend text\n            col.legend = tm_legend(title = \"\")) # No need for a legend title with a single raster variable\n\n\n\n\n\n\n\n\n\n\n\nTable\nI also want to be able to see a table of our exact suitable area in each region so we can use kable\n\n\nCode\nregion_suitable_area %&gt;% \n  st_drop_geometry() %&gt;% # The only way to lose geometry\n  select(-rgn_key, -rgn_id, -area_m2) %&gt;% # Drop unnecesary columns\nkbl(col.names = c('Region', 'Total Area (Km2)', 'Suitable Area (Km2)')) %&gt;%  # Rename the columns in our table\n  kable_classic(full_width = F, # This is my favorite type of table\n                html_font = \"Cambria\") %&gt;% \n  footnote(number = paste('EEZ region suitable area for farming Oysters')) # Add a footnote\n\n\n\n\n\nRegion\nTotal Area (Km2)\nSuitable Area (Km2)\n\n\n\n\nOregon\n179994.06\n1147.883\n\n\nNorthern California\n164378.81\n194.126\n\n\nCentral California\n202738.33\n3673.514\n\n\nSouthern California\n206860.78\n3259.485\n\n\nWashington\n66898.31\n2524.804\n\n\n\n1 EEZ region suitable area for farming Oysters"
  },
  {
    "objectID": "posts/aquaculture_and_eezs/index.html#function",
    "href": "posts/aquaculture_and_eezs/index.html#function",
    "title": "Aquaculture and EEZ’s",
    "section": "Function",
    "text": "Function\nNow that I have my workflow solidified I can turn it into a function, so with just the species name it can output a map.\n\n\nCode\n#' Map EEZ\n#' Make a map of a suitable area of a species in west coast EEZ's\n#' @param species \n#' The name of your species, MUST be wrapped AND Uppercase eg: ('Pacific Oyster')\n#' @returnsA list containing:\n#'   - $map   : tmap object\n#'   - $table : kable table\n#' @export\n#' @examples\n#' \nmap_eez &lt;- function(species){\n\n  species_dict &lt;- list(\n  \n  # -----------------------------\n  # SHELLFISH\n  # -----------------------------\n  \n  \"Pacific Oyster\" = list(\n    t_lwr = 11, t_upr = 30,\n    d_lower = 70, d_upr = 0\n  ),\n\n  \"Olympia Oyster\" = list(\n    t_lwr = 8,  t_upr = 20,\n    d_lower = 40, d_upr = 0\n  ),\n\n  \"Geoduck\" = list(\n    t_lwr = 5, t_upr = 18,\n    d_lower = 100, d_upr = 5\n  ),\n\n  \"Razor Clam\" = list(\n    t_lwr = 5, t_upr = 17,\n    d_lower = 20, d_upr = 0\n  ),\n\n  \"Blue Mussel\" = list(\n    t_lwr = 5, t_upr = 20,\n    d_lower = 30, d_upr = 0\n  ),\n\n  \"Mediterranean Mussel\" = list(\n    t_lwr = 8, t_upr = 26,\n    d_lower = 40, d_upr = 0\n  ),\n\n  \"Scallop\" = list(\n    t_lwr = 8, t_upr = 20,\n    d_lower = 50, d_upr = 0\n  ),\n  \n  # -----------------------------\n  # KELP + SEAWEEDS\n  # -----------------------------\n  \n  \"Bull Kelp\" = list(\n    t_lwr = 10, t_upr = 18,\n    d_lower = 40, d_upr = 0\n  ),\n  \n  \"Giant Kelp\" = list(\n    t_lwr = 6,  t_upr = 20,\n    d_lower = 30, d_upr = 0\n  ),\n  \n  \"Sugar Kelp\" = list(\n    t_lwr = -2, t_upr = 15,\n    d_lower = 50, d_upr = 1\n  ),\n  \n  \"Dulse\" = list(\n    t_lwr = 5,  t_upr = 15,\n    d_lower = 10, d_upr = 0\n  ),\n  \n  \"Sea Lettuce\" = list(\n    t_lwr = 5,  t_upr = 25,\n    d_lower = 10, d_upr = 0\n  ),\n  \n  \"Winged Kelp\" = list(\n    t_lwr = 2, t_upr = 15,\n    d_lower = 40, d_upr = 0\n  ),\n\n  \"Pyropia (Nori)\" = list(\n    t_lwr = 5,  t_upr = 15,\n    d_lower = 10, d_upr = 0\n  ),\n  \n  # -----------------------------\n  # ECHINODERMS (URCHINS)\n  # -----------------------------\n  \n  \"Green Sea Urchin\" = list(\n    t_lwr = 0, t_upr = 15,\n    d_lower = 200, d_upr = 0\n  ),\n\n  \"Red Sea Urchin\" = list(\n    t_lwr = 5, t_upr = 20,\n    d_lower = 100, d_upr = 3\n  ),\n  \n  # -----------------------------\n  # FINFISH\n  # -----------------------------\n\n  \"Chinook Salmon\" = list(\n    t_lwr = 8,  t_upr = 14,\n    d_lower = 200, d_upr = 0\n  ),\n\n  \"Coho Salmon\" = list(\n    t_lwr = 7, t_upr = 14,\n    d_lower = 150, d_upr = 0\n  ),\n\n  \"Steelhead Trout\" = list(\n    t_lwr = 7, t_upr = 17,\n    d_lower = 100, d_upr = 0\n  ),\n\n  \"Pacific Halibut\" = list(\n    t_lwr = 3,  t_upr = 12,\n    d_lower = 450, d_upr = 20\n  ),\n\n  \"Sablefish\" = list(\n    t_lwr = 4,  t_upr = 12,\n    d_lower = 1500, d_upr = 50\n  ),\n\n  \"Pacific Sardine\" = list(\n    t_lwr = 12, t_upr = 20,\n    d_lower = 0, d_upr = 50\n  ),\n\n  \"Northern Anchovy\" = list(\n    t_lwr = 12, t_upr = 20,\n    d_lower = 0, d_upr = 50\n  ),\n  \n  # -----------------------------\n  # CRUSTACEANS\n  # -----------------------------\n\n  \"Dungeness Crab\" = list(\n    t_lwr = 7, t_upr = 18,\n    d_lower = 100, d_upr = 0\n  )\n)\n\n# Extract parameters from the dictionary\nparams &lt;- species_dict[[species]]\nif (species == \"list\") {\n  return(names(species_dict))\n}\n\nif (is.null(params)) stop(\"Species not found in dictionary. Check spelling.\")\n\n# Access individual values\nt_lwr &lt;- params$t_lwr\nt_upr &lt;- params$t_upr\nd_lower &lt;- params$d_lower\nd_upr   &lt;- params$d_upr\n\n# Reclassification matrices\nrcl_depth &lt;- matrix(c(-Inf, -d_lower, 0, # Use variables like values\n                      -d_lower, -d_upr, 1,\n                      -d_upr, Inf, 0), \n                    nrow = 3, \n                    byrow = TRUE)\n\nrcl_temp &lt;- matrix(c(-Inf, t_lwr, 0,\n                     t_lwr, t_upr, 1,\n                     t_upr, Inf, 0), \n                   nrow =3,\n                   byrow = TRUE)\n\n# Reclassify \ndepth_rcl &lt;- classify(depth_resampled, rcl_depth)\n\nsst_rcl &lt;- classify(mean_sst, rcl_temp)\n\n# Calculate suitable cells\nsuitable_cells &lt;- sst_rcl * depth_rcl\n\nsuitable_cells[suitable_cells == 0] &lt;- NA\n\nnames(suitable_cells) &lt;- 'Suitable'\n\n# Calculate area of suitable cells\narea_raster &lt;- suitable_cells * cellSize(suitable_cells, unit = 'km')\n\n# Extract suitable areas \narea_extracted &lt;- terra::extract(area_raster, \n                                 eez, \n                                 touches = TRUE) %&gt;%  \n  group_by(ID) %&gt;% \n  summarise(suitable_area = sum(Suitable, na.rm = TRUE)) %&gt;% \n  mutate(rgn = eez$rgn)\n\n# Create geodf with suitable area and eez area\nregion_suitable_area &lt;- left_join(x = eez,\n                                  y = area_extracted, \n                                  by = \"rgn\") %&gt;% \n  select(-ID)\n\n\n# Create map\nmap &lt;- tm_basemap(\"Esri.WorldTopoMap\")+\n  tm_shape(region_suitable_area)+\n  tm_polygons(fill = 'suitable_area',\n              fill.scale = tm_scale(values = c('#E1DABD',\n                                               '#ABC798',\n                                               '#FFC4EB',\n                                               '#E6983F',\n                                               '#A0185A'),\n                                    n = 5), # Set 5 breaks\n              fill.legend = tm_legend( '        Total \\nSuitable area (km)'))+\n  tm_title(text = paste('Suitable area for farming', species, 'in West Coast EEZ'), position = tm_pos_out(\"center\", \"top\", pos.h = \"center\"))+ # Paste allows us to manipulate the title of the plot\n  tm_layout(component.autoscale = FALSE)+\n  tm_graticules(alpha = .5)+\n  tm_text('rgn', size = .7)+\ntm_shape(suitable_cells)+\n  tm_raster(col  =  'Suitable',\n             col.scale =  tm_scale(values = '#90AFDA', \n            labels = 'Suitable Area'),\n            col.legend = tm_legend(title = \"\"))\n\n# Create table\ntable &lt;- region_suitable_area %&gt;% \n  st_drop_geometry() %&gt;% \n  select(-rgn_key, -rgn_id, -area_m2) %&gt;% \nkbl(col.names = c('Region', \n                  'Total Area (Km2)', \n                  'Suitable Area (Km2)')) %&gt;% \n  kable_classic(full_width = F, \n                html_font = \"Cambria\") %&gt;% \n  footnote(number = paste('EEZ region suitable area for farming', species)) # Paste again\n\nreturn(list( # Make a list of our map and table so they can be called with a buck\n    map = map, \n    table = table\n  ))\n}\n\n\nNow we can call the function for any species in our species list\nTo know species options you can run map_eez(‘list’)\n\nmap_eez('list')\n\n [1] \"Pacific Oyster\"       \"Olympia Oyster\"       \"Geoduck\"             \n [4] \"Razor Clam\"           \"Blue Mussel\"          \"Mediterranean Mussel\"\n [7] \"Scallop\"              \"Bull Kelp\"            \"Giant Kelp\"          \n[10] \"Sugar Kelp\"           \"Dulse\"                \"Sea Lettuce\"         \n[13] \"Winged Kelp\"          \"Pyropia (Nori)\"       \"Green Sea Urchin\"    \n[16] \"Red Sea Urchin\"       \"Chinook Salmon\"       \"Coho Salmon\"         \n[19] \"Steelhead Trout\"      \"Pacific Halibut\"      \"Sablefish\"           \n[22] \"Pacific Sardine\"      \"Northern Anchovy\"     \"Dungeness Crab\"      \n\n\nNow we can call the function with any species from the list we find interesting, in this case Bull Kelp AND WHY\n\nbull_kelp &lt;- map_eez('Bull Kelp')\n\nCall the map\n\nbull_kelp$map\n\n\n\n\n\n\n\n\nCall the table\n\nbull_kelp$table\n\n\n\n\nRegion\nTotal Area (Km2)\nSuitable Area (Km2)\n\n\n\n\nOregon\n179994.06\n1065.055\n\n\nNorthern California\n164378.81\n1023.031\n\n\nCentral California\n202738.33\n1674.401\n\n\nSouthern California\n206860.78\n1622.344\n\n\nWashington\n66898.31\n3246.458\n\n\n\n1 EEZ region suitable area for farming Bull Kelp"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Vitale",
    "section": "",
    "text": "I’m a marine biologist with a passion for ocean life, data visualization, and science communication. I studied Marine Biology at UC Santa Cruz and have worked in environmental education, field research, and data analysis. I enjoy finding creative ways to connect people with the natural world and make environmental data more engaging and accessible."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "San Fransisco Bay Catch Dynamics\n\n\n\nMEDS\n\nR\n\nStatistics\n\n\n\nExamining Northern Anchovies ability to predict indicator species\n\n\n\nPeter Vitale\n\n\nDec 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the 2025 California Wildfires\n\n\n\nMEDS\n\nGeospatial\n\nPython\n\n\n\nPlotting true and false color images with landsat data.\n\n\n\nPeter Vitale\n\n\nDec 1, 2025\n\n\n\n\n\n\n\n\n\n\n\nAquaculture and EEZ’s\n\n\n\nMEDS\n\nR\n\nGeospatial\n\n\n\nExamining where in the North Pacific Economic Exclusive Zone is best for aquaculture of different species\n\n\n\nPeter Vitale\n\n\nNov 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtreme Weather Events\n\n\n\nMEDS\n\nR\n\nGeospatial\n\n\n\nExamining blackouts in Houston,TX following a 2021 storm\n\n\n\nPeter Vitale\n\n\nNov 12, 2025\n\n\n\n\n\n\nNo matching items"
  }
]